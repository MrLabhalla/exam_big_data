{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.2.3\n",
      "  Downloading pyspark-3.2.3.tar.gz (281.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.5 MB 38 kB/s  eta 0:00:01   |▊                               | 6.5 MB 423 kB/s eta 0:10:50     |██▋                             | 23.2 MB 537 kB/s eta 0:08:01     |████▌                           | 39.7 MB 683 kB/s eta 0:05:54     |████████████▋                   | 111.2 MB 605 kB/s eta 0:04:42     |████████████▊                   | 112.1 MB 514 kB/s eta 0:05:30     |█████████████████████▌          | 188.7 MB 275 kB/s eta 0:05:37██████        | 210.3 MB 944 kB/s eta 0:01:16     |████████████████████████        | 211.1 MB 1.3 MB/s eta 0:00:57     |████████████████████████        | 211.7 MB 1.3 MB/s eta 0:00:56     |████████████████████████▎       | 213.2 MB 639 kB/s eta 0:01:47     |████████████████████████▋       | 216.0 MB 848 kB/s eta 0:01:18     |█████████████████████████▎      | 222.8 MB 223 kB/s eta 0:04:23     |█████████████████████████▌      | 224.1 MB 682 kB/s eta 0:01:25     |██████████████████████████▉     | 236.0 MB 427 kB/s eta 0:01:47     |███████████████████████████     | 236.8 MB 394 kB/s eta 0:01:54     |███████████████████████████     | 237.4 MB 249 kB/s eta 0:02:57     |███████████████████████████▍    | 241.0 MB 640 kB/s eta 0:01:04     |█████████████████████████████▎  | 257.2 MB 262 kB/s eta 0:01:33     |█████████████████████████████▍  | 259.0 MB 403 kB/s eta 0:00:56     |█████████████████████████████▌  | 259.9 MB 350 kB/s eta 0:01:02     |█████████████████████████████▉  | 262.7 MB 248 kB/s eta 0:01:16\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.5 in /home/hdoop/.local/lib/python3.9/site-packages (from pyspark==3.2.3) (0.10.9.5)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.3-py2.py3-none-any.whl size=281990673 sha256=8593125adb0114c3670bd30df72d8b6cccf8e8cdad08aa0691ca9d50bac0f82d\n",
      "  Stored in directory: /home/hdoop/.cache/pip/wheels/cc/f4/8d/dfbbd536587311afde33711613a0c193f18e7d90b120801108\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.1\n",
      "    Uninstalling pyspark-3.3.1:\n",
      "      Successfully uninstalled pyspark-3.3.1\n",
      "Successfully installed pyspark-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/22 20:51:29 WARN Utils: Your hostname, boulloul-Latitude-E5470 resolves to a loopback address: 127.0.1.1; using 192.168.16.71 instead (on interface wlp1s0)\n",
      "23/01/22 20:51:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hdoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hdoop/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1ccc7408-b63e-47a1-99f9-f435afeab977;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 1579ms :: artifacts dl 87ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1ccc7408-b63e-47a1-99f9-f435afeab977\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/40ms)\n",
      "23/01/22 20:51:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/22 20:51:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[UDI: int, ProductID: string, Type: string, AirTemperature: double, ProcessTemperature: double, RotationalSpeed: int, Torque: double, ToolWear: int, Target: int, FailureType: string]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o82.save.\n: java.lang.UnsupportedOperationException: MongoCollection already exists\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:76)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     39\u001b[0m RotationAvg_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mRotationalSpeed\u001b[39m\u001b[39m\"\u001b[39m, \\\n\u001b[1;32m     40\u001b[0m    when(((df\u001b[39m.\u001b[39mRotationalSpeed \u001b[39m>\u001b[39m \u001b[39m1700\u001b[39m) \u001b[39m&\u001b[39m (df\u001b[39m.\u001b[39mRotationalSpeed \u001b[39m<\u001b[39m \u001b[39m1300\u001b[39m)), lit(\u001b[39m\"\u001b[39m\u001b[39mNormal\u001b[39m\u001b[39m\"\u001b[39m)) \\\n\u001b[1;32m     41\u001b[0m      \u001b[39m.\u001b[39motherwise(lit(\u001b[39m\"\u001b[39m\u001b[39mNot normal\u001b[39m\u001b[39m\"\u001b[39m)) \\\n\u001b[1;32m     42\u001b[0m   )\n\u001b[1;32m     46\u001b[0m \u001b[39m# store data in database \u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m avg_df\u001b[39m.\u001b[39;49mwrite\\\n\u001b[1;32m     50\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.mongodb.spark.sql.DefaultSource\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m     51\u001b[0m     \u001b[39m.\u001b[39;49moption( \u001b[39m\"\u001b[39;49m\u001b[39muri\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmongodb+srv://boulloul:Mehdi0601010046@cluster0.i3kx9.mongodb.net/Machine.average\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     52\u001b[0m     \u001b[39m.\u001b[39;49msave()\n\u001b[1;32m     54\u001b[0m failMtemperature_df\u001b[39m.\u001b[39mwrite\\\n\u001b[1;32m     55\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mcom.mongodb.spark.sql.DefaultSource\u001b[39m\u001b[39m'\u001b[39m)\\\n\u001b[1;32m     56\u001b[0m     \u001b[39m.\u001b[39moption( \u001b[39m\"\u001b[39m\u001b[39muri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmongodb+srv://boulloul:Mehdi0601010046@cluster0.i3kx9.mongodb.net/Machine.failMtemp\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     57\u001b[0m     \u001b[39m.\u001b[39msave()\n\u001b[1;32m     59\u001b[0m RotationAvg_df\u001b[39m.\u001b[39mwrite\\\n\u001b[1;32m     60\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mcom.mongodb.spark.sql.DefaultSource\u001b[39m\u001b[39m'\u001b[39m)\\\n\u001b[1;32m     61\u001b[0m     \u001b[39m.\u001b[39moption( \u001b[39m\"\u001b[39m\u001b[39muri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmongodb+srv://boulloul:Mehdi0601010046@cluster0.i3kx9.mongodb.net/Machine.RotationAvg\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     62\u001b[0m     \u001b[39m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:738\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m    739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o82.save.\n: java.lang.UnsupportedOperationException: MongoCollection already exists\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:76)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import when,lit\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"mongodbtest1\") \\\n",
    "    .master('local')\\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb+srv://boulloul:Mehdi0601010046@cluster0.i3kx9.mongodb.net/Machine\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://boulloul:Mehdi0601010046@cluster0.i3kx9.mongodb.net/Machine\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()    \n",
    "\n",
    "\n",
    "df = spark.read.csv(\"predictive_maintenances.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "#######################################################################\n",
    "#  computing and storing data as data source for grafana \n",
    "######################################################################\n",
    "\n",
    "# count the averge of each Type of Machine \n",
    "avgMachine = df.groupBy('Type').agg({'AirTemperature':'avg','ProcessTemperature':'avg','RotationalSpeed':'avg','Torque':'avg','ToolWear':'avg'})\n",
    "\n",
    "\n",
    "# Alert failure of a Machine (No : Air temperature<310; Yes : Air temperature>310)\n",
    "\n",
    "failtemperatureMachine = df.withColumn(\"AirTemperature\", \\\n",
    "   when((df.AirTemperature < 310), lit(\"No Need\")) \\\n",
    "     .when((df.AirTemperature == 310), lit(\"Failure Probab\")) \\\n",
    "     .otherwise(lit(\"Maintenance Needed\")) \\\n",
    "  ) \n",
    "\n",
    "# Machine RotationalSpeed (The average range from 1300 to 1700 ) \n",
    "\n",
    "RotationAvg_machines = df.withColumn(\"RotationalSpeed\", \\\n",
    "   when(((df.RotationalSpeed > 1700) & (df.RotationalSpeed < 1300)), lit(\"Normal\")) \\\n",
    "     .otherwise(lit(\"Not normal\")) \\\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "# store data in database (as data sour e for grafana) managred by mongodb\n",
    "\n",
    "avgMachine.write\\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "    .option( \"uri\", \"mongodb+srv://labhalla:Mehdi0601010046@cluster0.i3kx9.mongodb.net/avgMachine\") \\\n",
    "    .save()\n",
    "\n",
    "failtemperatureMachine.write\\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "    .option( \"uri\", \"mongodb+srv://labhalla:Mehdi0601010046@cluster0.i3kx9.mongodb.net/failtemperatureMachine\") \\\n",
    "    .save()\n",
    "\n",
    "RotationAvg_machines.write\\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "    .option( \"uri\", \"mongodb+srv://boulloul:Mehdi0601010046@cluster0.i3kx9.mongodb.net/RotationAvg_machines\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
